{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4e9bb2",
   "metadata": {},
   "source": [
    "# Lab 4 â€” Zarzadzanie zrodlami danych strumieniowych i segmentacja klientow\n",
    "\n",
    "## ðŸ”„ Wprowadzenie\n",
    "\n",
    "W tym laboratorium zapoznasz sie z roznymi metodami zasilania danych strumieniowych w Apache Spark oraz zastosowaniem prostych transformacji, filtrowania i segmentacji klientow w czasie rzeczywistym.\n",
    "\n",
    "## ðŸ’¡ Pomocnicza funkcja do wyswietlania naszych danych strumieniowych\n",
    "\n",
    "Funkcja ta pozwala wyÅ›wietlaÄ‡ batche strumieni w notatniku. Dodatkowo ograniczona zostaÅ‚a do realizacji 5 nastÄ™pujÄ…cych po sobie elementÃ³w. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717728c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_counter[\"count\"] % 5 == 0:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18ef5ae",
   "metadata": {},
   "source": [
    "## ðŸ”¹ rate jako ÅºrÃ³dÅ‚o kontrolowanego strumienia\n",
    "\n",
    "### âœ… Zadanie 1\n",
    "\n",
    "1. Przygotuj strumien danych z `format('rate')`, ustaw `rowsPerSecond` na 5.\n",
    "2. Utworz kolumne `user_id`: `expr(\"concat('u', cast(rand()*100 as int))\")`\n",
    "3. Dodaj kolumne `event_type`: `expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4679f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "rate_df = (spark.readStream\n",
    "           .format(\"rate\")\n",
    "           .option(\"rowsPerSecond\", 5)\n",
    "           .load())\n",
    "\n",
    "events = rate_df.withColumn(\"user_id\", expr(\"concat('u', cast(rand()*100 as int))\")) \\\n",
    "                 .withColumn(\"event_type\", expr(\"case when rand() > 0.7 then 'purchase' else 'view' end\"))\n",
    "\n",
    "query = (events.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26572a27",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Å¹rÃ³dÅ‚o plikowe (JSON)\n",
    "\n",
    "### âœ… Schemat danych:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user_id\": \"u123\",\n",
    "  \"event_type\": \"purchase\", // albo \"view\", \"cart\", \"click\"\n",
    "  \"timestamp\": \"2025-05-09T15:24:00Z\",\n",
    "  \"product_id\": \"p456\",\n",
    "  \"category\": \"electronics\",\n",
    "  \"price\": 299.99\n",
    "}\n",
    "```\n",
    "\n",
    "### âœ… Generator danych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430b1c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file generator.py\n",
    "# generator.py\n",
    "import json, os, random, time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "output_dir = \"data/stream\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "event_types = [\"view\", \"cart\", \"purchase\"]\n",
    "categories = [\"electronics\", \"books\", \"fashion\", \"home\", \"sports\"]\n",
    "\n",
    "def generate_event():\n",
    "    return {\n",
    "        \"user_id\": f\"u{random.randint(1, 50)}\",\n",
    "        \"event_type\": random.choices(event_types, weights=[0.6, 0.25, 0.15])[0],\n",
    "        \"timestamp\": (datetime.utcnow() - timedelta(seconds=random.randint(0, 300))).isoformat(),\n",
    "        \"product_id\": f\"p{random.randint(100, 120)}\",\n",
    "        \"category\": random.choice(categories),\n",
    "        \"price\": round(random.uniform(10, 1000), 2)\n",
    "    }\n",
    "\n",
    "# Simulate file-based streaming\n",
    "while True:\n",
    "    batch = [generate_event() for _ in range(50)]\n",
    "    filename = f\"{output_dir}/events_{int(time.time())}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        for e in batch:\n",
    "            f.write(json.dumps(e) + \"\\n\")\n",
    "    print(f\"Wrote: {filename}\")\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881ff9eb",
   "metadata": {},
   "source": [
    "### Schemat danych "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02685d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aa6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"data/stream\"))\n",
    "\n",
    "# sprawdz opcje \n",
    "# .option(\"maxFilesPerTrigger\", 2) \n",
    "# .option(\"header\", true) for csv file \n",
    "\n",
    "query = (stream.writeStream\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2c07f",
   "metadata": {},
   "source": [
    "### Okna czasowe z wykorzystaniem Watermark\n",
    "\n",
    "1. Tumbling window\n",
    "2. Sliding window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f7d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])\n",
    "\n",
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"data/stream\"))\n",
    "\n",
    "windowed = (stream.withWatermark(\"timestamp\", \"1 minute\") \n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\", \"1 minute\"), \"event_type\")\n",
    "    .count())\n",
    "\n",
    "query = (windowed.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .format(\"console\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3317974",
   "metadata": {},
   "source": [
    "## przykÅ‚ad - segmentacja klientÃ³w "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503312a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"StreamingDemo\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"event_type\", StringType()),\n",
    "    StructField(\"timestamp\", TimestampType()),\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"category\", StringType()),\n",
    "    StructField(\"price\", DoubleType())\n",
    "])\n",
    "\n",
    "stream = (spark.readStream\n",
    "          .schema(schema)\n",
    "          .json(\"data/stream\"))\n",
    "\n",
    "\n",
    "user_events = (stream\n",
    "    .withWatermark(\"timestamp\", \"1 minute\")\n",
    "    .groupBy(window(\"timestamp\", \"10 seconds\"), \"user_id\")\n",
    "    .agg(collect_set(\"event_type\").alias(\"events\")))\n",
    "\n",
    "segmented = user_events.withColumn(\"segment\", expr(\"\"\"\n",
    "    CASE\n",
    "        WHEN array_contains(events, 'purchase') THEN 'Buyer'\n",
    "        WHEN array_contains(events, 'cart') THEN 'Cart abandoner'\n",
    "        WHEN array_contains(events, 'view') THEN 'Lurker'\n",
    "        ELSE 'Other'\n",
    "    END\n",
    "\"\"\"))\n",
    "\n",
    "query = (segmented.writeStream\n",
    "         .outputMode(\"append\")\n",
    "         .format(\"console\")\n",
    "         .foreachBatch(process_batch)\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f6c230",
   "metadata": {},
   "source": [
    "### przesylanie strumienia przez socket \n",
    "\n",
    "```bash\n",
    "nc -lk 9998\n",
    "\n",
    "\n",
    "ps | grep 9998\n",
    "kill -9 <p_id>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file start_stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"MobyDick.txt\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9998\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27e3cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "    if batch_counter[\"count\"] % 5 == 0:\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf923aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9998)\n",
    "         .load())\n",
    "\n",
    "    words = lines.select(explode(split(lines.value, \" \")).alias(\"words\"))\n",
    "    word_counts = words.groupBy(\"words\").count()\n",
    "\n",
    "    streamingQuery = (word_counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\") # .outputMode(\"update\")\n",
    "#         .trigger(processingTime=\"5 second\")\n",
    "         .start())\n",
    "\n",
    "    streamingQuery.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890497bb",
   "metadata": {},
   "source": [
    "# Przetwarzanie danych strumieniowych z Apache Kafka i Spark\n",
    "\n",
    "1. SprawdÅº czy serwer Kafki posiada jakieÅ› zdefiniowane topici:\n",
    "    - w dodatkowym oknie termianala wpisz polecenie:\n",
    "    ```bash\n",
    "    cd ~ \n",
    "    kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n",
    "    ```\n",
    "2. dodaj topic o nazwie `streamXX` Gdzie za `X` wstaw nr grupy a za `xx` nr swojego serwera\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streamXX\n",
    "```\n",
    "\n",
    "3. sprawdÅº listÄ™ tematÃ³w ponownie upewniajÄ…c siÄ™, Å¼e posiadasz temat `streamXX`\n",
    "\n",
    "4. Uruchom nowy terminal na swoim komputerze i utwÃ³rz producenta generujÄ…cego dane do nowego topicu\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic stream\n",
    "```\n",
    "\n",
    "Aby sprawdziÄ‡ czy wysyÅ‚anie wiadomoÅ›ci dziaÅ‚a uruchom kolejne okno terminala i wpisz nastÄ™pujÄ…cÄ… komendÄ™ realizujÄ…cÄ… konsumenta: \n",
    "\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-console-consumer.sh  --bootstrap-server broker:9092 --topic streamXX \n",
    "```\n",
    "Zweryfikuj, Å¼e przesyÅ‚anie danych dziaÅ‚a. \n",
    "\n",
    "Zamknij okno producenta. Okno konsumenta zostaw otwarte - przyda siÄ™ do weryfikacji automatu generujÄ…cego dane. \n",
    "\n",
    "## Uruchomienie kodu wysyÅ‚ajÄ…cego strumieÅ„\n",
    "\n",
    "UzupeÅ‚nij skrypt tak by generowaÅ‚ nastÄ™pujÄ…ce dane: \n",
    "\n",
    "1. utwÃ³rz zmiennÄ… `message` ktÃ³ra bÄ™dzie sÅ‚ownikiem zawierajÄ…cym informacje pojedynczego eventu (klucz: wartoÅ›Ä‡): \n",
    "    - \"time\" : aktualny czas w postaci stringu datetime.now()\n",
    "    - \"id\" : wybierane losowo z listy [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    - \"value: losowa wartoÅ›Ä‡ z zakresu 0 do 100\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57afa4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file stream.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "KAFKA_SERVER = \"broker:9092\"\n",
    "TOPIC = 'stream'\n",
    "LAG = 2\n",
    "\n",
    "def create_producer(server):\n",
    "    return KafkaProducer(\n",
    "        bootstrap_servers=[server],\n",
    "        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n",
    "        api_version=(3, 7, 0),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    producer = create_producer(KAFKA_SERVER)\n",
    "    try:\n",
    "        while True:\n",
    "\n",
    "            message = {\n",
    "                \"time\" : str(datetime.now() )  ,\n",
    "                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"])     ,\n",
    "                \"temperatura\" : random.randint(-100,100)  ,\n",
    "                \"cisnienie\" :  random.randint(0,50)   ,\n",
    "            }\n",
    "  \n",
    "            producer.send(TOPIC, value=message)\n",
    "            sleep(LAG)\n",
    "    except KeyboardInterrupt:\n",
    "        producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1678a8a",
   "metadata": {},
   "source": [
    "2.  w terminalu jupyterlab uruchom plik `stream.py`\n",
    "```bash\n",
    "python stream.py\n",
    "```\n",
    "\n",
    "sprawdz w oknie consumenta czy wysyÅ‚ane wiadomoÅ›ci przychodzÄ… do Kafki.\n",
    "\n",
    "Za uruchomienie importu kafka odpowiedzialna jest biblioteka `kafka-python`\n",
    "ktÃ³rÄ… moÅ¼esz zainstalowaÄ‡ poleceniem `pip install kafka-python`\n",
    "\n",
    "## APACHE SPARK \n",
    "\n",
    "Przygotuj kod skryptu ktÃ³ry pobierze informacje z przesyÅ‚anego strumienia danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad77181",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file app.py\n",
    "\n",
    "# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 app.py\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "TOPIC = 'stream'\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"temperatura\", IntegerType()),\n",
    "            StructField(\"cisnienie\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "SCHEMA = \"\"\"time Timestamp, id String, temperatura Int, cisnienie Int \"\"\" # DDL string\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "     \n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", SERVER)\n",
    "        .option(\"subscribe\", TOPIC)\n",
    "        .load()\n",
    "    # .select(col(\"value\").cast(\"string\"))\n",
    "    )\n",
    "    # query =  (\n",
    "    #     raw.writeStream\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    parsed = (raw.select(\"timestamp\", from_json(decode(col(\"value\"), \"utf-8\"), SCHEMA).alias(\"moje_dane\"))\n",
    "                .select(\"timestamp\", \"moje_dane.*\")\n",
    "             )\n",
    "    # query =  (\n",
    "    #     parsed.writeStream\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    # gr = parsed.agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n",
    "    # query =  (gr.writeStream\n",
    "    #     .outputMode(\"update\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    gr = (parsed.withWatermark(\"timestamp\", \"5 seconds\")\n",
    "    .groupBy(window(\"timestamp\", \"10 seconds\", \"7 seconds\"))\n",
    "    .agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n",
    "         )\n",
    "    query =  (gr.writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", False)\n",
    "        .start()\n",
    "    )\n",
    "    query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ca2c89-c745-4f46-8602-a3349b27c807",
   "metadata": {},
   "source": [
    "# Apache Spark z plikami RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5381f-7d0b-4de0-be1e-72a8e64db9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f21b5e-d0e7-4c4c-add9-004e3dcf91dc",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "- Resilient Distributed Dataset\n",
    "- Podstawowa abstrakcja oraz rdzeń Sparka\n",
    "- Obsługiwane przez dwa rodzaje operacji:\n",
    "    - Akcje:\n",
    "        - operacje uruchamiająceegzekucję transformacji na RDD\n",
    "        - przyjmują RDD jako input i zwracają wynik NIE będący RDD\n",
    "    - Transformacje:\n",
    "        - leniwe operacje\n",
    "        - przyjmują RDD i zwracają RDD\n",
    "\n",
    "- In-Memory - dane RDD przechowywane w pamięci\n",
    "- Immutable \n",
    "- Lazy evaluated\n",
    "- Parallel - przetwarzane równolegle\n",
    "- Partitioned - rozproszone \n",
    "\n",
    "## WAŻNE informacje !\n",
    "\n",
    "Ważne do zrozumienia działania SPARKA:\n",
    "\n",
    "Term                   |Definition\n",
    "----                   |-------\n",
    "RDD                    |Resilient Distributed Dataset\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "\n",
    "\n",
    "Dwie podstawowe metody tworzenia RDD:\n",
    "\n",
    "Method                      |Result\n",
    "----------                               |-------\n",
    "`sc.parallelize(array)`                  |Create RDD of elements of array (or list)\n",
    "`sc.textFile(path/to/file)`                      |Create RDD of lines from file\n",
    "\n",
    "Podstawowe transformacje\n",
    "\n",
    "Transformation Example                          |Result\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "Podstawowe akcje \n",
    "\n",
    "Action                             |Result\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd4f25-1b77-4c00-9bbc-ee8327581059",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['Books', 'DVD', 'CD', 'PenDrive'] # nasze dane \n",
    "\n",
    "key_rdd = sc.parallelize(keywords) # metoda parallelize - \"wczyta dane\" \n",
    "\n",
    "key_rdd.collect() # akcja wyświetlania "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5716e7-51af-4828-8f5c-eb48cf39329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_small = key_rdd.map(lambda x: x.lower()) # transformacja\n",
    "\n",
    "key_small.collect() # akcja "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2827b6-a724-410d-91a0-e24aae226bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8204ee-ec92-40ae-92ec-c612138aca52",
   "metadata": {},
   "source": [
    "## Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a61951-804f-4a3b-b753-666b496b1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"new\").getOrCreate()\n",
    "\n",
    "# otrzymanie obiektu SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fc53bb-1ffd-4b21-be04-2b06af0808cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tekst = sc.textFile(\"MobyDick.txt\")\n",
    "tekst.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab50e81-46a6-4554-b138-fa3777b57f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Word Count on RDD \n",
    "sc.textFile(\"MobyDick.txt\")\\\n",
    ".map(lambda x: re.findall(r\"[a-z']+\", x.lower())) \\\n",
    ".flatMap(lambda x: [(y, 1) for y in x]).reduceByKey(lambda x,y: x + y)\\\n",
    ".take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f34798-8cc4-49b0-b2f8-8fb53a5652aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e40055-4582-4b32-941a-e31d70d96a94",
   "metadata": {},
   "source": [
    "## SPARK STREAMING\n",
    "\n",
    "Część Sparka odpowiedzialna za przetwarzanie danych w czasie rzeczywistym. \n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-arch.png\"/>\n",
    "\n",
    "Dane mogą pochodzić z różnych źródeł np. sokety TCP, Kafka, etc. \n",
    "Korzystając z poznanych już metod `map, reduce, join, oraz window` można w łatwy sposób generować przetwarzanie strumienia tak jaby był to nieskończony ciąg RDD. \n",
    "Ponadto nie ma problemu aby wywołać na strumieniu operacje ML czy wykresy. \n",
    "\n",
    "Cała procedura przedstawia się następująco: \n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-flow.png\"/>\n",
    "\n",
    "SPARK STREAMING w tej wersji wprowadza abstrakcje zwaną `discretized stream` *DStream* (reprezentuje sekwencję RDD).\n",
    "\n",
    "Operacje na DStream można wykonywać w API JAVA, SCALA, Python, R (nie wszystkie możliwości są dostępne dla Pythona). \n",
    "\n",
    "Spark Streaming potrzebuje minium 2 rdzenie.\n",
    "\n",
    "----\n",
    "- **StreamingContext(sparkContext, batchDuration)** - reprezentuje połączenie z klastrem i służy do tworzenia DStreamów, `batchDuration` wskazuje na granularność batch'y (w sekundach)\n",
    "- **socketTextStream(hostname, port)** - tworzy DStream na podstawie danych napływających ze wskazanego źródła TCP\n",
    "- **flatMap(f), map(f), reduceByKey(f)** - działają analogicznie jak w przypadku RDD z tym że tworzą nowe DStream'y\n",
    "- **pprint(n)** - printuje pierwsze `n` (domyślnie 10) elementów z każdego RDD wygenerowanego w DStream'ie\n",
    "- **StreamingContext.start()** - rozpoczyna działania na strumieniach\n",
    "- **StreamingContext.awaitTermination(timeout)** - oczekuje na zakończenie działań na strumieniach\n",
    "- **StreamingContext.stop(stopSparkContext, stopGraceFully)** - kończy działania na strumieniach\n",
    "\n",
    "Obiekt StreamingContext można wygenerować za pomocą obiektu SparkContext.\n",
    "\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream.png\"/>\n",
    "\n",
    "<img src=\"https://spark.apache.org/docs/latest/img/streaming-dstream-ops.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3f7ea5-78df-40ee-8d13-ebeff4a2f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a local StreamingContext with two working thread\n",
    "# and batch interval of 1 second\n",
    "\n",
    "sc = SparkContext(\"local[2]\", \"NetworkWordCount2\")\n",
    "ssc = StreamingContext(sc, 2)\n",
    "\n",
    "# DStream\n",
    "lines = ssc.socketTextStream(\"localhost\", 9998)\n",
    "\n",
    "# podziel każdą linię na wyrazy\n",
    "# DStream jest mapowany na kolejny DStream\n",
    "# words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "\n",
    "words = lines.flatMap(lambda x: re.findall(r\"[a-z']+\", x.lower()))\n",
    "\n",
    "# zliczmy każdy wyraz w każdym batchu\n",
    "# DStream jest mapowany na kolejny DStream\n",
    "# pairs = words.map(lambda word: (word, 1))\n",
    "\n",
    "# DStream jest mapowany na kolejny DStream                  \n",
    "# wordCounts = pairs.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "wordCounts = words.map(lambda word: (word,1)).reduceByKey(lambda x,y: x+y)\n",
    "# wydrukuj pierwszy element\n",
    "wordCounts.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9514ca8-f7ea-4114-ab5f-09f2fb41da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# w konsoli linuxowej netcat Nmap for windows\n",
    "!nc -lk 9998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258e6d8d-126f-4d29-94a3-c91d138c8b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before starting, run a stream data\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()\n",
    "ssc.stop()\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32242bbd-fc7f-4bc4-b904-fc6c399676a4",
   "metadata": {},
   "source": [
    "### przesylanie strumienia przez socket \n",
    "\n",
    "```bash\n",
    "nc -lk 9998\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4078e7-d090-45ab-9484-f8e31b4d4823",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file start_stream.py\n",
    "\n",
    "from socket import *\n",
    "import time\n",
    "\n",
    "rdd = list()\n",
    "with open(\"MobyDick.txt\", 'r') as ad:\n",
    "    for line in ad:\n",
    "        rdd.append(line)\n",
    "\n",
    "HOST = 'localhost'\n",
    "PORT = 9998\n",
    "ADDR = (HOST, PORT)\n",
    "tcpSock = socket(AF_INET, SOCK_STREAM)\n",
    "tcpSock.bind(ADDR)\n",
    "tcpSock.listen(5)\n",
    "\n",
    "\n",
    "while True:\n",
    "    c, addr = tcpSock.accept()\n",
    "    print('got connection')\n",
    "    for line in rdd:\n",
    "        try:\n",
    "            c.send(line.encode())\n",
    "            time.sleep(1)\n",
    "        except:\n",
    "            break\n",
    "    c.close()\n",
    "    print('disconnected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd4ab29-3ea0-4706-b84a-5b209b4ced71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file streamWordCount.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "    lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9998)\n",
    "         .load())\n",
    "\n",
    "    words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "    word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "    streamingQuery = (word_counts\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\")\n",
    "         .trigger(processingTime=\"5 second\")\n",
    "         .start())\n",
    "\n",
    "    streamingQuery.awaitTermination()\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b485a8-4d34-49b3-96ce-712b24112cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "batch_counter = {\"count\": 0}\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    batch_counter[\"count\"] += 1\n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Stream_DF\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "    \n",
    "lines = (spark\n",
    "         .readStream\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9998)\n",
    "         .load())\n",
    "\n",
    "words = lines.select(explode(split(lines.value, \" \")).alias(\"word\"))\n",
    "word_counts = words.groupBy(\"word\").count()\n",
    "\n",
    "streamingQuery = (word_counts.writeStream\n",
    "         .format(\"console\")\n",
    "         .outputMode(\"complete\")\n",
    "         .foreachBatch(process_batch) \n",
    "         .trigger(processingTime=\"5 second\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbf1595-90b8-496e-b68d-ea7c0c335470",
   "metadata": {},
   "source": [
    "# Przetwarzanie danych strumieniowych\n",
    "\n",
    "1. Sprawdź czy serwer Kafki posiada jakieś zdefiniowane topici:\n",
    "    - w dodatkowym oknie termianala wpisz polecenie:\n",
    "    ```bash\n",
    "    cd ~ \n",
    "    kafka/bin/kafka-topics.sh --list --bootstrap-server broker:9092\n",
    "    ```\n",
    "2. dodaj topic o nazwie `streamXX` Gdzie za `X` wstaw nr grupy a za `xx` nr swojego serwera\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-topics.sh --bootstrap-server broker:9092 --create --topic streamXX\n",
    "```\n",
    "\n",
    "3. sprawdź listę tematów ponownie upewniając się, że posiadasz temat `streamXX`\n",
    "\n",
    "4. Uruchom nowy terminal na swoim komputerze i utwórz producenta generującego dane do nowego topicu\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-console-producer.sh --bootstrap-server broker:9092 --topic stream\n",
    "```\n",
    "\n",
    "Aby sprawdzić czy wysyłanie wiadomości działa uruchom kolejne okno terminala i wpisz następującą komendę realizującą konsumenta: \n",
    "\n",
    "```bash\n",
    "cd ~ \n",
    "kafka/bin/kafka-console-consumer.sh  --bootstrap-server broker:9092 --topic streamXX \n",
    "```\n",
    "Zweryfikuj, że przesyłanie danych działa. \n",
    "\n",
    "Zamknij okno producenta. Okno konsumenta zostaw otwarte - przyda się do weryfikacji automatu generującego dane. \n",
    "\n",
    "## Uruchomienie kodu wysyłającego strumień\n",
    "\n",
    "Uzupełnij skrypt tak by generował następujące dane: \n",
    "\n",
    "1. utwórz zmienną `message` która będzie słownikiem zawierającym informacje pojedynczego eventu (klucz: wartość): \n",
    "    - \"time\" : aktualny czas w postaci stringu datetime.now()\n",
    "    - \"id\" : wybierane losowo z listy [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "    - \"value: losowa wartość z zakresu 0 do 100\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9368ff49-580d-4507-8bfc-95586b9fd791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file stream.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "import sys\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "\n",
    "KAFKA_SERVER = \"broker:9092\"\n",
    "TOPIC = 'stream'\n",
    "LAG = 2\n",
    "\n",
    "def create_producer(server):\n",
    "    return KafkaProducer(\n",
    "        bootstrap_servers=[server],\n",
    "        value_serializer=lambda x: json.dumps(x).encode(\"utf-8\"),\n",
    "        api_version=(3, 7, 0),\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    producer = create_producer(KAFKA_SERVER)\n",
    "    try:\n",
    "        while True:\n",
    "\n",
    "            message = {\n",
    "                \"time\" : str(datetime.now() )  ,\n",
    "                \"id\" : random.choice([\"a\", \"b\", \"c\", \"d\", \"e\"])     ,\n",
    "                \"temperatura\" : random.randint(-100,100)  ,\n",
    "                \"cisnienie\" :  random.randint(0,50)   ,\n",
    "            }\n",
    "  \n",
    "            producer.send(TOPIC, value=message)\n",
    "            sleep(LAG)\n",
    "    except KeyboardInterrupt:\n",
    "        producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ff505-9591-4254-a1f4-270e0e68622f",
   "metadata": {},
   "source": [
    "2.  w terminalu jupyterlab uruchom plik `stream.py`\n",
    "```bash\n",
    "python stream.py\n",
    "```\n",
    "\n",
    "sprawdz w oknie consumenta czy wysyłane wiadomości przychodzą do Kafki.\n",
    "\n",
    "Za uruchomienie importu kafka odpowiedzialna jest biblioteka `kafka-python`\n",
    "którą możesz zainstalować poleceniem `pip install kafka-python`\n",
    "\n",
    "## APACHE SPARK \n",
    "\n",
    "Przygotuj kod skryptu który pobierze informacje z przesyłanego strumienia danych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f779145-e8db-462f-97dc-538a40438177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "%%file app.py\n",
    "\n",
    "# spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1 app.py\n",
    "\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "SERVER = \"broker:9092\"\n",
    "TOPIC = 'stream'\n",
    "\n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"time\", TimestampType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"temperatura\", IntegerType()),\n",
    "            StructField(\"cisnienie\", IntegerType()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "SCHEMA = \"\"\"time Timestamp, id String, temperatura Int, cisnienie Int \"\"\" # DDL string\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"WARN\")\n",
    "     \n",
    "    raw = (\n",
    "        spark.readStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", SERVER)\n",
    "        .option(\"subscribe\", TOPIC)\n",
    "        .load()\n",
    "    )\n",
    "    # query =  (\n",
    "    #     raw.writeStream\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    parsed = (raw.select(\"timestamp\", from_json(decode(col(\"value\"), \"utf-8\"), SCHEMA).alias(\"moje_dane\"))\n",
    "                .select(\"timestamp\", \"moje_dane.*\")\n",
    "             )\n",
    "    # query =  (\n",
    "    #     parsed.writeStream\n",
    "    #     .outputMode(\"append\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    # gr = parsed.agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n",
    "    # query =  (gr.writeStream\n",
    "    #     .outputMode(\"update\")\n",
    "    #     .format(\"console\")\n",
    "    #     .option(\"truncate\", False)\n",
    "    #     .start()\n",
    "    # )\n",
    "    gr = (parsed.withWatermark(\"timestamp\", \"5 seconds\")\n",
    "    .groupBy(window(\"timestamp\", \"10 seconds\", \"7 seconds\"))\n",
    "    .agg(avg(\"temperatura\"), avg(\"cisnienie\"))\n",
    "         )\n",
    "    query =  (gr.writeStream\n",
    "        .outputMode(\"complete\")\n",
    "        .format(\"console\")\n",
    "        .option(\"truncate\", False)\n",
    "        .start()\n",
    "    )\n",
    "    query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420b610-51d9-4974-b66d-b3d74a65e7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
